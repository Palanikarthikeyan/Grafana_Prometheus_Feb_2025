Recap
------
promQL =>metric based

<metricName> {K=Value,K2 =~RegxPatter}[duration] ->rangevector 
			->rate() ; aggregate_based function() ->Instant vector

----------------
Application monitoring
-------------------------
\__ 1. install Prometheus library 
\__ 2. import <or> include <or> source Prometheus library
\__ 3. create own custom metric //class<-->object ; functionCall() ; ..
		  --------------
write our code 		|->Counter Gauge Histogram Summary
|
port:8000
|
|------------<-----------Prometheus:9090
------------------

+---------------------------------+
| [Flask:5000               ]
|  |->mysql3306;web:8080;tools
+---------------------------------+
	| 
    [port:8000]
	|-------------------------------------------Prometheus:9090
  curl 127.0.0.1:5000 ->like www.abc.com /
       
  curl 127.0.0.1:5000/aboutus
  ....
--------------------------------------------------------------------------------------
alert manager 
--------------
 alert manager - executable file 
 --------------
  |->manages alerts generated by Prometheus
  |->invoke the channels

----------------------
1st - create an alert rule (like recording rule ->p1.yml)
----------------------
   |->update to prometheus.yml file 

\__ 1. download alertmanager binary from Prometheus.io/downloads
\__ 2. extract - unzip
\__ 3. create alertmanager.yml file
	      ------------------
		\__4. write channel configuration
		    if you do any changes in alertmanager.yml - restart alertmanager 
\__ 5. start alertmanager
		|->R+ 

\___6. In prometheus (prometheus.yml) file ->under alertmanager key
			remove the #   
			- alertmanager:9093
\__7. restart prometheus

===================================================================
alert rule -> yaml file 
-----------------------

groups:
- name: <groupName>
  rules:
  - record: ruleName
    expr: promQL
  - alert: alertrule
    expr: promQL - condition based  
    for: duration
    labels:
      severity: <critical;warining;High;low;ticket>
    annotations:
      summary:  user defined message

we can combine recording rule and alert rule
each rule will be separate index 
-----------------------------------------------------------------------
groups:
- name: demoAlert
  rules:
  - alert: cpuUsage
    expr: (avg by(instance)(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) <95
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: instance under heavy load
-----------------------------------------------------------------------
Channel Configuration
---------------------
step 1: https://company.com/slack.com/apps
|
step 2: [manage] <--Goto manage
|
step 3: customeintegration
|
step 4: Incoming webhooks
|
step 5: add a configuration
|
step 6: choose channel
|
step 7: webhook_URL: https://hooks.slack.... <== copy this line
|
step 8: paste it into alertmanager.yml
                      slack_configs:
	              - api_url: 'https://' <=== paste
|
step 9: restart your alertmanager
-------------------------------------------------------------------------------
		
global:
  resolve_timeout: 1m
  slack_api_url: 'https://hooks.slack.com/services/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'

route:
  group_by: ['alertname']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 1h
  receiver: 'slack-notifications'
receivers:
- name: 'slack-notifications'
  slack_configs:
  - channel: '#testprom'
    send_resolved: true
inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'dev', 'instance']

--------------------------------------------------------------------------------------

Service Discovery (SD)
------------------
|
|->1. prometheus - to scrape metrics using static configuration via static_configs:
								    ----------------
|->2. for dynamic env - where new instance ; ip add getting updated //SD

|->3. before SD ->Configuration Management Technique (like: ansible)

CM ->[inventory_file]
	- list of remote node
	...
	[webserver] <== group
	node1
	node2
	[devserver] <== group
	node3
	node4
	[QAserver] <== group
	node5
	node3
		
  prometheus read this inventory file - get list of all nodes

  scrape_configs:
  - job_name: node
    static_configs:
      - targets:
           {% for var in groups["all"]%}
		- {{var}:9100
	   {% endfor %}
  ------------------------------------------------
  scrape_configs:
  - job_name: node
    static_configs:
      - targets: ["10.20.30.40:9100"]
      - targets: ["10.23.32.34:9100"]
      - targets: ["10.33.43.32:9100"]


node_cpu_total
	{instance:"10.20.30.40:9100", job:"node"}
	{instance:"10.23.32.34:9100", job:"node"}
	{instance:"10.33.43.32:9100", job:"node"} 
					 // common job name
-------------------------------------------------------------------------------------
File Server Discovery 
---------------------
fileSD
--------
 |-> does not network
 |-> read monitoring targets from file
 |-> json 
 |-> metadata __<name>__ 
 |
 |->Prometheus restart is not required 
 
 scrape_configs:
  - job_name: "file" 
    file_sd_configs:
     - files:
         - 'p1.json'
	      <or>
	 - '*.json'
------------------------------

service discovery activity
--------------------------
1. stop prometheus.exe (or) systemctl stop prometheus.service 
|
2. backup an existing prometheus.yml file => backup_prometheus.yml
|
3. edit prometheus.yml
	|
	scrape_configs:
        - job_name: "file"
          file_sd_configs:
          - files:
             - '*.json'
 
   save this prometheus.yml
 |
 4. start this prometheus.exe (or) systemctl start prometheus 

L=[{"targets":["130.61.219.247:9100"],"labels":{"job":"Cloud-OL8"}}]

import json
wobj = open("f1.json","w")
json.dump(L,wobj)
wobj.close()
----------------------------
|
   Go to prometheus.yml file location => keep your python code(p1.py)
					 run this python code
|
5. run the above python code ->f1.json
| 
|  dir (or) ls 
|  # see f1.json file is exists
|
6. Go to prometheus targets -> refresh browswer


		
file:p1.py
------------
import json
L=[{"targets":["192.168.1.5:9091"],"labels":{"job":"VM-Ubuntu-pushgateway"}}]
wobj = open("f3.json","w")
json.dump(L,wobj)
wobj.close()
----------------

##############################################################################

Consul
 |
 |-> SD mechanism - allows application to automatically find and communicate
 |-> micro service 
 |-> CM
  ...
  ...
  ....

consul_sd_configs:
- server: 'IP of consulServer:<port>'
  services: ['']

download consul agent
|
unzip 
|
./consul agent -dev
 <localhost::<port>>

------------------------

Old style
-----------
scrape_configs:
- job_name: ec2
  ec2_sd_configs:
    - region: <region>
      accesskey: <accesskey>
      secretkey: <secret key>
.....



__meta__ec2_instance_id_ = ""
...
|
aws_<metric>
---------------------------------------------------------------
Container - monitoring
---------------------
cgroups - kernel subsystem

CGI:  web server DB
   ----------------------
	
	Kernel
   ----------------------
	H/W
	
	Vs
CGI:  web |server | DB
     ---|----|------|----
      [LXC][LXC]  [LXC] <== sub-system - cgroup
	  Kernel
     -------------------

.service  - process
.slice    - blue of service
 =====
  |-->.system/
		..... systemctl -t service <== list of services

  |-->.user/
	  |->userA - 1001
			|----->/.... 
	  |->userB - 1002
			|----->/...

  |->machine/
	

container_cpu_load_average....{id="/system.slice/NetworkManager.service....} 
container_cpu_load_average....{id="/system.slice/sshd.service....} 
....

cAdvisor - opensource tool by Google

curl 127.0.0.1:<port>/metrics

OL8  - cloud instance
----
https://github.com/google/cadvisor/releases

download cadv 
|
give execute permission # chmod a+x <filename>
./filename

open another terminal 
-> curl 127.0.0.1:8080/metrics
   ...
#####################################################################
kubernetes
------------
|-minikube - run a single-node kubernetes cluster
---------------------------------
curl -LO https://github.com/kubernetes/minikube/releases/latest/download/minikube-linux-amd64
---------------------------------
1. Donwload and run - minikube
# To demonstrate using prometheus with kubernetes

   wget https://storage.googleapis.com/minikube/releases/v0.24.1/minikuber-linux-amd64
2. Execute minikube
   mv minikube-linux-amd64 minikube  # rename 
   chmod +x minikube    # give execute permission
   ./minikube start     # run a minikube
starting local kubernetes v1.8.....
starting vm
..
   ./minikube dashboard --url # URL for kubernetes Dashboard 

# Commandline-tool 
3. To interact with cluster - kubectl
wget https://storage.googleapis.com/kubernetes-release/release/v<version>..

wget https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
 
 chmod +x kubectl {Enter}
 ./kubectl get services  {Enter}
Name  TYPE  CLUSER-IP ... 
		|
	     <IP>
		
# How to get an example prometheus running on minikube
# deplyment.yml file - permission (prometheus can access the resources(pods,nodes))

4. kubectl apply -f prometheus-deployment.yml {Enter}  (Optional)
		   ----------------------------
			
	
5. ./minikube service prometheus --url {enter}
http://192.168.99.100:<port>
|
service discovery
|
scrape_configs:
- job_name: ''
  kubernetes_sd_configs:
   - role: pod
=======================================================================
prometheus
 - localhost
 - remotenode 
 ...
 ...
 exporter - executable file  - running - collect the metrics ->promQL 
 .........
fileSD 
- .... <=== prometheus 
- ....
-----------------------------------------------------------------------
Administration
Organization - Grafana - Instance 
 |->Team
      |->sales
           |->user1 user2 user3
      |->QA
           |->userA,userB

login: admin 
	|->can create datasource,dashboard,edit - the query,dashboard 
				 install plugin etc.,

roles
------
admin role - create new user ; can't create datasource 
editor role - create dashboard - view - edit panel attributes ; not datasource 
view role - view only 
------------------------------------------------------

grafana + prometheus => measurment (float)
		- metric model 
		- promQL
	   - by SoundCloud 
....
--------------------------------------------------------

grafana + loki => log aggregation (text)
	  ----
	   |->this is not measurment 
	   |-by Grafana Labs
	   |->interface (log broswer) look like prometheus interface 
	


Grafana Loki - log aggregation system
	     - text based 
	     - stores and query the logs
	
Loki Components
----------------
1.Loki 
2.promtail
3.grafana

[App]->[Log]  <-- |
		  | [ promtail ] ----->[loki] ---->Grafana - Visualize logs 
[App]->[Log]  <-- |


package management - install 
 (or)
download and setup 

promtail
loki
https://github.com/grafana/loki/releases
https://raw.githubusercontent.com/grafana/loki/master/cmd/loki/loki-local-config.yaml
https://github.com/grafana/loki/blob/main/clients/cmd/promtail/promtail-local-config.yaml

loki-config.yml
promtail-config.yml

./loiki-  -config=loki-config.yml
./promtail -config=promtail-local-config.yaml
			.......
---------------------------------

Grafana reference book link:
-----------------------------
https://drive.google.com/file/d/1Gndt38W8hddhghEOn-LvY9_nuhS1Ijos/view?usp=drive_link
################################################################################



